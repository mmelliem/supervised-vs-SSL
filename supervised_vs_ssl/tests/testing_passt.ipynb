{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20c9049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n",
      "tensor([[ -3.6805,  -8.3676,  -8.6802,  -8.6212,  -9.8335,  -7.7412, -10.5780,\n",
      "          -8.9576,  -9.4711,  -9.6206,  -8.3532,  -8.8844,  -9.3976,  -9.9513,\n",
      "          -9.0039,  -9.8462,  -8.9856, -10.6377, -10.5501, -10.3609, -10.6368,\n",
      "         -10.1024,  -9.8688,  -9.0761,  -9.6175,  -9.9393, -10.6421,  -6.5953,\n",
      "          -7.6170, -10.4024,  -8.5895,  -8.8322,  -8.6766,  -8.3217,  -8.6720,\n",
      "          -9.6039,  -9.2695, -11.8473,  -8.4319,  -8.2881,  -8.9780,  -9.6865,\n",
      "         -12.1173,  -9.8753, -10.5931, -11.7499, -10.1343, -10.3481, -10.2758,\n",
      "          -9.0025, -11.8408,  -9.7058,  -7.4914,  -8.7267,  -9.4040,  -9.7281,\n",
      "         -15.4620, -10.6928, -10.1274, -10.0498,  -8.9604,  -9.5768, -10.4931,\n",
      "          -8.1220,  -9.7426,  -9.9354,  -8.2156,  -9.4181, -10.6371,  -7.2337,\n",
      "          -8.2600,  -9.9473,  -6.1599,  -7.2340,  -7.5427,  -9.7209,  -9.1217,\n",
      "         -11.3660,  -8.5717, -10.5072,  -8.9862,  -9.0461,  -8.8459,  -9.7928,\n",
      "          -5.7675, -10.3266,  -8.2710, -10.0480, -10.4945, -11.5021, -10.3145,\n",
      "         -11.1524, -11.5832, -10.2798,  -8.7906, -10.5480,  -9.3769,  -9.6559,\n",
      "          -8.7559,  -8.8690,  -8.8598,  -9.7443, -10.0354,  -9.9044,  -8.6491,\n",
      "          -8.4373, -11.0025, -10.0070,  -9.6463,  -9.6298,  -8.9927,  -7.2308,\n",
      "          -8.5500,  -9.0948, -13.0750,  -9.2482,  -9.2620,  -9.4257,  -8.8202,\n",
      "          -9.9752, -11.9221, -11.1178,  -8.7031,  -9.1681,  -8.6284, -10.2732,\n",
      "          -9.3133, -10.5730, -11.0408, -10.3749,  -6.8806, -10.3025,  -8.6377,\n",
      "         -10.3010,  -8.6912,  -7.0590, -10.9168,  -3.7914,  -6.2282,  -7.4425,\n",
      "          -7.5566,  -8.6389,  -8.8068,  -8.9225, -10.4185, -10.9049,  -8.1352,\n",
      "          -9.5658, -10.4696, -10.0967, -10.0438,  -9.9373,  -8.5436,  -8.1293,\n",
      "          -9.4308, -10.0272, -10.4252, -10.5440,  -7.4857,  -7.2258, -10.9773,\n",
      "          -7.1352,  -7.4957,  -8.5368,  -6.4895,  -7.9800,  -8.6959,  -8.9837,\n",
      "          -7.6850,  -9.8718,  -9.3695,  -8.4840,  -9.0033,  -9.4992,  -9.8304,\n",
      "          -8.9116,  -9.5828, -11.3920,  -8.9341,  -9.6550,  -9.4604, -10.1379,\n",
      "         -10.5212, -10.2296,  -7.7654,  -8.8071, -10.0106,  -9.3557, -10.1224,\n",
      "          -8.1148, -10.3453,  -7.0574,  -8.4398,  -8.9442,  -8.9476,  -8.5476,\n",
      "          -8.6533,  -9.2239, -10.7377,  -9.7500,  -8.7001, -10.2890, -10.0483,\n",
      "         -12.9316, -12.4448, -10.5194, -10.3500, -10.2396,  -9.7957, -10.2028,\n",
      "         -10.5646,  -8.6527, -11.1852,  -9.0961, -10.1494,  -9.1849,  -8.5063,\n",
      "          -7.9566,  -8.4806,  -7.9453,  -7.4191,  -8.3485,  -9.6052,  -8.7953,\n",
      "          -7.9528,  -9.7962,  -8.3062,  -9.4301,  -9.3645,  -9.1148, -10.3401,\n",
      "          -9.4649,  -8.8676,  -9.6556,  -8.8828,  -8.4377,  -8.7577,  -7.7574,\n",
      "          -9.8355,  -6.6392,  -8.9374,  -7.3840,  -7.4543,  -7.5345,  -8.8178,\n",
      "          -8.3582,  -8.8199,  -9.2310,  -9.3117,  -9.5364,  -9.3259,  -8.3462,\n",
      "         -10.0380,  -9.3493,  -7.6142,  -9.3272,  -9.5281,  -9.7773,  -9.5905,\n",
      "          -9.0099,  -8.8031,  -9.2673, -10.9875, -10.4709,  -9.6577,  -8.9608,\n",
      "          -8.7370,  -9.9799,  -8.0660,  -9.4282,  -8.1903,  -8.1897, -10.1577,\n",
      "          -9.8831,  -8.5220,  -9.7304, -10.3872,  -9.0411,  -9.8452,  -8.4720,\n",
      "          -8.5419,  -7.4540,  -7.9210,  -7.3291,  -6.1870,  -7.3150,  -8.3995,\n",
      "          -9.3026,  -7.3209,  -6.7626,  -6.6603,  -6.0294,  -7.2598,  -5.2051,\n",
      "          -9.1366,  -9.4113,  -5.7849,  -8.2251,  -8.6792,  -7.1140,  -5.5428,\n",
      "          -8.2507,  -9.8684,  -9.5065,  -7.9946,  -8.7485,  -7.9587,  -6.6276,\n",
      "          -8.7879, -10.7340, -11.4826, -12.8787, -10.0633,  -9.9497,  -8.2960,\n",
      "          -8.3450,  -8.4034,  -9.0941, -10.0867, -10.6911, -11.8704,  -8.7344,\n",
      "          -9.7043,  -9.6163, -10.6516,  -9.9519,  -8.6150, -10.2333,  -7.5252,\n",
      "          -7.5154,  -9.5031,  -9.2778,  -7.4962, -11.0528,  -8.7752,  -6.9241,\n",
      "          -7.9744,  -5.7541,  -7.1631,  -9.4450,  -6.9881,  -9.0937,  -9.3715,\n",
      "          -5.9071,  -9.5940, -10.3083, -10.2439,  -9.5533,  -7.2929,  -5.7901,\n",
      "          -9.3384,  -9.8557,  -6.4273,  -7.1911,  -9.3472, -11.2224, -11.7025,\n",
      "         -10.9931,  -8.8785,  -9.4044,  -7.7136,  -9.6087, -11.6059, -11.0797,\n",
      "         -10.0574, -10.6885, -10.7390,  -7.8675,  -7.4388, -10.2180,  -8.9438,\n",
      "          -9.6243, -10.6577,  -8.7153, -10.4248, -11.8678,  -8.0659,  -9.8336,\n",
      "          -7.8439, -10.6808, -11.6716, -10.2369,  -7.3591, -10.2976, -10.0719,\n",
      "         -10.2044, -10.2006,  -7.8491,  -9.8450,  -9.5214, -12.0884, -10.4912,\n",
      "         -10.0995,  -9.5865, -11.0203,  -9.5005,  -8.6348,  -9.6850,  -8.2539,\n",
      "         -10.8468,  -9.8203, -10.9564,  -8.9849,  -8.6947,  -7.7931,  -8.7276,\n",
      "          -9.5134,  -8.0892,  -8.0070,  -9.3643, -12.5703,  -8.6901, -10.5297,\n",
      "          -9.1222, -11.7565,  -9.3410, -10.9710,  -9.3750,  -7.6667,  -9.2172,\n",
      "          -9.9164,  -9.4163, -10.3439, -10.4985,  -7.7857,  -7.5771,  -8.7868,\n",
      "          -9.2451,  -9.9572,  -9.0471,  -9.2999, -10.8136,  -8.2380,  -8.7761,\n",
      "          -7.9802,  -8.3517, -10.2129,  -9.3137,  -9.2430, -12.8363,  -8.6730,\n",
      "          -7.8127,  -9.0874, -11.4193,  -8.3909, -11.6590,  -8.7414,  -8.5358,\n",
      "          -9.2170, -10.6070, -11.2158,  -9.5430,  -9.0296,  -5.4349,  -7.6788,\n",
      "         -10.1491,  -9.6549, -10.3288,  -9.8279,  -6.8124,  -8.1473,  -8.8597,\n",
      "         -10.0644,  -9.5499, -10.4790, -10.5977,  -8.3023,  -8.6901,  -8.7650,\n",
      "          -9.2939,  -8.5599,  -9.6364,  -6.5619,  -6.9315,  -8.3051,  -6.9524,\n",
      "          -8.3266,  -8.1943, -11.8677,  -9.2930,  -9.8269,  -8.1084,  -7.9197,\n",
      "         -10.6712, -10.6598,  -9.5751, -11.4145,  -5.2045,  -5.6812,  -5.6225,\n",
      "          -7.3672, -10.1936,  -9.7059,  -8.7347,  -9.3099, -10.1915,  -5.6192,\n",
      "         -10.2736,  -9.7731,  -9.8144,  -7.0345,  -8.3980,  -7.6089,  -9.6040,\n",
      "          -4.7551,  -9.0815,  -4.6564,  -5.0530,  -7.5745,  -5.7610,  -5.6447,\n",
      "          -6.7173,  -7.1678,  -2.1023, -11.6284,   1.1510,  -6.1128,  -9.3055,\n",
      "          -9.5326,  -4.5787,   0.2221,  -3.4644,  -7.1215,  -6.3344,  -7.7193,\n",
      "          -7.3296,  -8.0632]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from hear21passt.base import get_basic_model, get_model_passt\n",
    "\n",
    "# get the passt model tensor\n",
    "model = get_basic_model(mode=\"logits\")\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "# test it with a random tensor\n",
    "audio = torch.randn((1, 32000*10)).cuda()\n",
    "# remove gradient to not use much processing power on little test\n",
    "with torch.no_grad():\n",
    "    output = model(audio)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77873515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# make sure that model runs on GPU, should say \"cuda:0\"\n",
    "print(next(model.parameters()).device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcas_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
